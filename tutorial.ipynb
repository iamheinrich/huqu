{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6e3ced",
   "metadata": {},
   "source": [
    "# Define the pipeline config\n",
    "\n",
    "This requires some knowledge about the dataset. It's important to note that we expect the dataset to have a train and test split. If it doesn't, you will have to adapt `huqu.stages.dataset_loading.py` accordingly.\n",
    "\n",
    "But don't worry, we have defined an example pipeline for you already. \n",
    "\n",
    "## Default pipeline config\n",
    "\n",
    "```yaml\n",
    "dataset:\n",
    "  path: \"Bingsu/Cat_and_Dog\"  # The path of the hugging face dataset\n",
    "  config_name: \"default\"\n",
    "  class_name: \"dog\"\n",
    "  class_label: 1\n",
    "  label_key: \"labels\"  # Some datasets have different keys for the labels, like \"label\" or \"labels\"\n",
    "  num_train_samples: 2\n",
    "  num_test_samples: 2\n",
    "  main_subject: \"dogs\"  # The main theme or subject of the dataset\n",
    "  captions_path: \"data/captions_dogs.parquet\"  # Path to store the captions DataFrame\n",
    "  assignments_path: \"data/assignments_dogs.parquet\"  # Path to store the assignments DataFrame\n",
    "  unrefined_criteria_path: \"data/unrefined_criteria_dogs.json\"  # Path to store initial criteria\n",
    "  refined_criteria_path: \"data/refined_criteria_dogs.json\"  # Path to store refined criteria\n",
    "  format: \"parquet\"  # Format for storing DataFrames\n",
    "  compression: \"snappy\"  # Compression method for parquet files\n",
    "\n",
    "stages:\n",
    "  criteria_init:\n",
    "    batch_size: 2\n",
    "  criteria_refinement:\n",
    "    num_rounds: 1\n",
    "    sample_size: 2\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccebb5",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "Once the pipeline is configured you can load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd858fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huqu.stages.dataset_loading import DatasetLoadingStage\n",
    "dataset_loader = DatasetLoadingStage()\n",
    "dataset = dataset_loader.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34757d05",
   "metadata": {},
   "source": [
    "# Creating captions\n",
    "Once the dataset is loaded, we can load a model of your choice an created the captions.\n",
    "For executing this cell, you need an openAI key that is stored in `.env` as `OPENAI_API_KEY=<your-api-key>`\n",
    "\n",
    "The captions will automatically be saved to a new folder called `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # First, we need to initialize the model.\n",
    " from huqu.models.chatgpt import GPT4oMiniMLLM\n",
    " multimodal_model = GPT4oMiniMLLM()\n",
    " \n",
    " # Now we can kick-off the caption generation.\n",
    " from huqu.stages.caption_generation import CaptionGenerationStage\n",
    " caption_generator = CaptionGenerationStage(multimodal_model)\n",
    " caption_generator.process(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f16e8",
   "metadata": {},
   "source": [
    "# Criteria Initialization\n",
    "Now we need to create the criteria, i.e. the dimensions and attributes. The unrefined criteria will be stored in a `unrefined_criteria_path` that you defined in `pipeline_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # First, we need to initialize the model.\n",
    "from huqu.models.chatgpt import GPT4oMiniLLM\n",
    "text_model = GPT4oMiniLLM()\n",
    "\n",
    "# Now we can initialize the criteria.\n",
    "from huqu.stages.criteria_initilization import CriteriaInitializationStage\n",
    "criteria_initializer = CriteriaInitializationStage(text_model)\n",
    "criteria_initializer.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af6107",
   "metadata": {},
   "source": [
    "# Criteria Initialization\n",
    "Now we need to refine the criteria. The refined criteria will be stored in a `refined_criteria_path` that you defined in `pipeline_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c06ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huqu.stages.criteria_refinement import CriteriaRefinementStage\n",
    "criteria_refiner = CriteriaRefinementStage(text_model)\n",
    "criteria = criteria_refiner.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f6fab",
   "metadata": {},
   "source": [
    "# Image assignment stage\n",
    "Now we succesfully discovered and refined the relevant subgroups for the class defined in the pipeline config. It's time to assign each captions - which represents the image - to a subgroup. In practice that means, we assign each caption to one attribute per dimension. \n",
    "\n",
    "The result will be stored in `assignments_path` that you have defined in your `pipeline_config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a300865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huqu.stages.image_assignment import ImageAssignmentStage\n",
    "\n",
    "image_assignment = ImageAssignmentStage(text_model)\n",
    "image_assignment.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414255e8",
   "metadata": {},
   "source": [
    "## ðŸ¥³ You have discovered and assigned the subpopulations for the first class. Now it's time to run the pipeline for another class.\n",
    "\n",
    "### To do that, we simply need to adapt the pipeline config\n",
    "\n",
    "Please open `pipeline_config.yaml` and paste the following parameters to run the pipeline for the class cat.\n",
    "\n",
    "\n",
    "```yaml\n",
    "dataset:\n",
    "  path: \"Bingsu/Cat_and_Dog\"  # The path of the hugging face dataset\n",
    "  config_name: \"default\"\n",
    "  class_name: \"cat\"\n",
    "  class_label: 0\n",
    "  label_key: \"labels\"  # Some datasets have different keys for the labels, like \"label\" or \"labels\"\n",
    "  num_train_samples: 2\n",
    "  num_test_samples: 2\n",
    "  main_subject: \"cat\"  # The main theme or subject of the dataset\n",
    "  captions_path: \"data/captions_cats.parquet\"  # Path to store the captions DataFrame\n",
    "  assignments_path: \"data/assignments_cats.parquet\"  # Path to store the assignments DataFrame\n",
    "  unrefined_criteria_path: \"data/unrefined_criteria_cats.json\"  # Path to store initial criteria\n",
    "  refined_criteria_path: \"data/refined_criteria_cats.json\"  # Path to store refined criteria\n",
    "  format: \"parquet\"  # Format for storing DataFrames\n",
    "  compression: \"snappy\"  # Compression method for parquet files\n",
    "\n",
    "stages:\n",
    "  criteria_init:\n",
    "    batch_size: 2\n",
    "  criteria_refinement:\n",
    "    num_rounds: 1\n",
    "    sample_size: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae05f49",
   "metadata": {},
   "source": [
    "# Running the pipeline again.\n",
    "Now, we can simply run all pipeline stages again. They will pull from the updated pipeline config automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_loader.process()\n",
    "caption_generator.process(dataset)\n",
    "criteria_initializer.process()\n",
    "criteria_refiner.process()\n",
    "image_assignment.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926492a",
   "metadata": {},
   "source": [
    "# ðŸ¥³ You have now run the pipeline for two classes.\n",
    "\n",
    "## But it's not the end yet. \n",
    "\n",
    "There is still a bit of work to do before we can analyze the results. In fact, we need to create a training and a test set for both classes we analyzed. We have created a script that does that for you.\n",
    "\n",
    "The only thing you have to make sure is that the `main` function of `merge_datasets.py` uses the right parameters. Setting the right parameters should be self-explanatory. You can infer them again from `pipeline_config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d13edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.merge_datasets import main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a0ce5-9930-4c82-8c87-97dc035b19fb",
   "metadata": {},
   "source": [
    "# Now it's time to analyze the subpopulations\n",
    "## To do that, we first load the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f85b9034-71a0-48a4-8477-2aa8450196b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_test = \"dogs_and_cats_test.parquet\"\n",
    "#parquet_file_test = \"compost_and_metal_test.parquet\"\n",
    "#parquet_file_test = \"fighting_and_laughing_test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042d5d80-b60a-4a93-ab07-90f63e2c478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_train = \"dogs_and_cats_train.parquet\"\n",
    "#parquet_file_train = \"compost_and_metal_train.parquet\"\n",
    "#parquet_file_train = \"fighting_and_laughing_train.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b0cd7-745b-4bc1-87f3-abb6a0821e7a",
   "metadata": {},
   "source": [
    "## Combine dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f97d0dd-156c-471f-8d4d-11933762645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(parquet_file_train)\n",
    "df_test = pd.read_parquet(parquet_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff4ea9b-0955-4e15-9042-855df352856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'train': df_train,\n",
    "    'test': df_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3484a-1387-4740-8c1a-f8b68e7cfbcb",
   "metadata": {},
   "source": [
    "## Configure and Initialize the Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4eb823-52b0-46a0-97d1-89b4e5ad5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optional custom configuration\n",
    "# custom_config = {\n",
    "#     'over_threshold': 0.6,       # Flag attributes appearing in more than X% of instances\n",
    "#     'under_threshold': 0.05,     # Flag attributes appearing in less than X% of instances\n",
    "#     'figure_size': (14, 8),      # Set larger figure size\n",
    "#     'rare_threshold': 5          # Consider attributes appearing less than X times as rare\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523903bb-bc98-4b3c-bfaa-418dfd4dd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer with our data and custom config\n",
    "analyzer = DataAnalyzer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a2f8c-7d3d-4bbe-936a-9e85189e5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive report including intraclass and interclass analysis\n",
    "analyzer.complete_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15194fe4-3ca7-42a1-b6cf-f883ce735434",
   "metadata": {},
   "source": [
    "## Focused Analysis: Intra-Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690a795-635d-4932-a9ff-e16332baf203",
   "metadata": {},
   "source": [
    "The `detect_outliers()` function identifies any attributes that are overrepresented or underrepresented across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebd807-9653-4569-963d-92c27d110162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in attribute distribution\n",
    "outliers = analyzer.intra.detect_outliers()\n",
    "display(outliers.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc33c1-9a21-491c-8c7d-87bd3392e3ec",
   "metadata": {},
   "source": [
    "The `get_class_outliers()` function identifies any attributes that are overrepresented or underrepresented for a **specific class**. \n",
    "\n",
    "Adjust `over_threshold` and `under_threshold` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f842b74-f3d3-48ba-9de6-46dfc41199af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific class\n",
    "dog_outliers = analyzer.intra.get_class_outliers(\"dog\")\n",
    "display(dog_outliers.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8eefe-1f3d-4aa0-892a-9bee8f8a04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "analyzer.intra.plot_histogram(dog_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3c4ed-208e-49d6-a513-caae39865fcc",
   "metadata": {},
   "source": [
    "The `get_dimension_outliers()` function identifies any attributes that are overrepresented or underrepresented for a **specific class-dimension pair**. \n",
    "\n",
    "Adjust `over_threshold` and `under_threshold` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ad510-478e-4552-beb4-7833a6907d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific dimension within a class\n",
    "dog_size_outliers = analyzer.intra.get_dimension_outliers(\"dog\", \"size\")\n",
    "display(dog_size_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa16c9b-c1af-42f5-afa9-00b64e405c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "analyzer.intra.plot_histogram(dog_size_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368999a-e1b4-4750-a04f-4b67472fd03d",
   "metadata": {},
   "source": [
    "## Focused Analysis: Inter-Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d1218-896d-46e7-8288-9f107d7b2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rare attributes\n",
    "rare_attrs = analyzer.inter.analyze_rare_attributes(threshold=4)\n",
    "display(rare_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533e0a5-548d-4e93-b331-e07d3540d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attributes across classes\n",
    "missing_attrs = analyzer.inter.compare_class_attributes()\n",
    "display(missing_attrs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca27cd-5b72-43bf-99c4-88a10b38b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attributes missing in a specific class\n",
    "dog_missing = analyzer.inter.get_unique_to_class(\"dog\")\n",
    "display(dog_missing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huqu-ETNb1EPG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
